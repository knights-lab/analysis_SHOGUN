{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from itertools import cycle\n",
    "from skbio.stats.composition import clr\n",
    "from scipy.spatial.distance import dice\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.spatial.distance import dice\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "sns.palplot(sns.color_palette(\"colorblind\", 10))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COLORS=dict(zip(['BURST 95', 'BURST 98', 'BURST 95 Taxonomy', 'BURST 98 Taxonomy',\n",
    "       'UTree', 'Bowtie2 95', 'Bowtie2 98', \"Kraken\", \"Centrifuge\"], sns.color_palette(\"husl\", 9)))\n",
    "DEPTHS = ['0001', '001', '01', '1', 'fulldepth']\n",
    "COLUMNS = ['aligner', 'depth', 'tp', 'fp', 'fn', 'precision', 'recall', 'f1', 'level', 'site', 'spearman', 'pearson', 'jaccard']\n",
    "\n",
    "# DEPTHS_DICT\n",
    "CLR = True\n",
    "# CLASSIFIER = lambda: OneVsRestClassifier(RandomForestClassifier(n_estimators=12))\n",
    "CLASSIFIER = lambda: OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=True))\n",
    "CLASSES = [\"tongue_dorsum\", \"stool\", \"supragingival_plaque\", \"right_retroauricular_crease\", \n",
    "           \"left_retroauricular_crease\", \"subgingival_plaque\"]\n",
    "CLASSES_MAP = dict(zip(CLASSES, (\"oral\", \"stool\", \"oral\", \"skin\", \"skin\", \"oral\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_plot(fig, pltname, artists=()):\n",
    "    fig.savefig(os.path.join(\"..\", \"figures\", \"simulations_\" + pltname + \".png\"), dpi=300, bbox_extra_artists=artists, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the fasta depths of each\n",
    "from collections import defaultdict\n",
    "DEPTHS_DICT = defaultdict(int)\n",
    "files = glob.glob(\"../results/simulations_all_95/*1/count.seqs.txt\")\n",
    "for file in files:\n",
    "    sequence_depth = !cat {file}\n",
    "    depth = file.split('/')[-2].split('_')[-2]\n",
    "    DEPTHS_DICT[depth] += int(sequence_depth[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'0001': 3036,\n",
       "             '001': 29978,\n",
       "             '01': 300143,\n",
       "             '1': 2999770,\n",
       "             'fulldepth': 30000003})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEPTHS_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up the list of taxonomic profilers\n",
    "taxonomic_profilers = ['UTree', 'BURST 98', 'BURST 98 Taxonomy', 'Bowtie2', 'Kraken', 'Centrifuge', 'BURST 95',  'BURST 95 Taxonomy', 'Bowtie 95']\n",
    "# Select BURST capitalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up the ground truth taxatable\n",
    "ground_truth_species = pd.read_csv(\"../results/simulations/taxatable.strain.species.normalized.txt\", sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head2tax = dict()\n",
    "with open(\"../data/references/rep82/rep82.tax\") as tax_inf:\n",
    "    for line in tax_inf:\n",
    "        line = line.rstrip().split('\\t')\n",
    "        head2tax[line[0]] = line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load up sequence to simulations\n",
    "title2acc = dict()\n",
    "title2tax = dict()\n",
    "with open(\"../results/simulations/combined_seqs.fna\") as fh:\n",
    "    for line in fh:\n",
    "        if line.startswith(\">\"):\n",
    "            row = line.split()\n",
    "            title = row[0][1:]\n",
    "            accession = '_'.join(row[1].split('_')[:2])\n",
    "            title2acc[title] = accession\n",
    "            title2tax[title] = head2tax[accession]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_taxatable(df):\n",
    "    # Features X Samples\n",
    "    # Redistribute to median depth\n",
    "    df = df.div(df.sum(axis=0).div(df.sum(axis=0).median()), axis=1).round().astype(int)\n",
    "    df_temp = df.apply(lambda x: x/x.sum(), axis=0)\n",
    "    df = df[df_temp.apply(lambda x: x.mean(), axis=1) >= .002]\n",
    "    df[df == 0] = .65\n",
    "    df = df.apply(lambda x: x/x.sum(), axis=0)\n",
    "    data = df.T.values\n",
    "    data = clr(data)\n",
    "    return pd.DataFrame(data, index=df.columns, columns=df.index)\n",
    "\n",
    "def subset_taxatable(df):\n",
    "    # Redistribute to median depth\n",
    "    df = df.div(df.sum(axis=0).div(df.sum(axis=0).median()), axis=1).round().astype(int)\n",
    "    df_temp = df.apply(lambda x: x/x.sum(), axis=0)\n",
    "    df = df[df_temp.apply(lambda x: x.mean(), axis=1) >= .002]\n",
    "    return df\n",
    "\n",
    "def normalize_two_taxatable(df_found, df_gold):\n",
    "    # Features X Samples\n",
    "    df_found = subset_taxatable(df_found)\n",
    "    df_gold = subset_taxatable(df_gold)\n",
    "    df_found = df_found.T\n",
    "    df_found = df_found[[_ in df_gold.columns for _ in df_found.index]].T\n",
    "    df_found.columns = [_ + \"_found\" for _ in df_found.columns]\n",
    "#     df_found['group'] = [\"found\" for _ in range(df_found.shape[0])]\n",
    "#     df_gold['group'] = [\"gold\" for _ in range(df_gold.shape[0])]\n",
    "    df_joined = df_gold.join(df_found)\n",
    "    df_joined = df_joined.fillna(0)\n",
    "    df_joined[df_joined == 0] = .65\n",
    "    df_joined = df_joined.apply(lambda x: x/x.sum(), axis=0)\n",
    "    data = df_joined.T.values\n",
    "    data = clr(data)\n",
    "    df_clr =  pd.DataFrame(data, index=df_joined.columns, columns=df_joined.index)\n",
    "    return df_clr[['found' in _ for _ in df_clr.index]], df_clr[['found' not in _ for _ in df_clr.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GOLD_SPECIES = normalize_taxatable(ground_truth_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up the taxonomies for BURST\n",
    "def fetch_burst_results():\n",
    "    # Columns = [aligner, depth, tp, fp, fn, precision, recall, f1, level, site, spearman, pearson, jaccard]\n",
    "    # Load up the tp/fp\n",
    "    for depth in DEPTHS:\n",
    "        for alignment in ['95', '98']:\n",
    "            file_str = \"../results/simulations_all_{alignment}/combined_seqs.fna_{depth}_*/alignment.burst.b6\".format(depth=depth, alignment=alignment)\n",
    "            infiles = glob.glob(file_str)\n",
    "            for infile in infiles:\n",
    "                f1_results = defaultdict(lambda: defaultdict(int))\n",
    "                with open(infile) as fh:\n",
    "                    for line in fh:\n",
    "                        row = line.rstrip().split(\"\\t\")\n",
    "                        accession_found = row[1]\n",
    "                        accession_actual = title2acc[row[0]]\n",
    "                        group = row[0].split('_')[0]\n",
    "                        if accession_found == accession_actual:\n",
    "                            f1_results[group]['tp'] += 1\n",
    "                        else:\n",
    "                            f1_results[group]['fp'] += 1\n",
    "                folder = '/'.join(infile.split('/')[:-1])\n",
    "                df = pd.read_csv(folder + \"/taxatable.burst.species.normalized.txt\", sep=\"\\t\", index_col=0)\n",
    "                df_norm, df_gold = normalize_two_taxatable(df, ground_truth_species)\n",
    "                for group in f1_results.keys():\n",
    "                    taxahit_found = set(df[df[group] > 5].index)\n",
    "                    taxahit_actual = set(ground_truth_species[ground_truth_species[group] > 5].index)\n",
    "                    jaccard = len(taxahit_found.intersection(taxahit_actual))/float(len(taxahit_found.union(taxahit_actual)))\n",
    "                    tp = f1_results[group]['tp']\n",
    "                    fp = f1_results[group]['fp']\n",
    "                    fn = int(DEPTHS_DICT[depth]/3.)-(tp+fp)\n",
    "                    precision = tp/float(tp+fp)\n",
    "                    recall = tp/float(tp+fn)\n",
    "                    f1_score = (precision+recall)/2.\n",
    "                    level = 'species'\n",
    "                    site = group\n",
    "                    series_found = df_norm.T[group + \"_found\"]\n",
    "                    series_actual = df_gold.T[group]\n",
    "                    pearson = series_actual.corr(series_found, method='pearson')\n",
    "                    spearman = series_actual.corr(series_found, method='spearman')\n",
    "#                     df_temp = pd.DataFrame([series_found, series_actual]).T\n",
    "#                     df_temp.columns = ['found', 'actual']\n",
    "#                     df_temp = df_temp.fillna(0)\n",
    "#                     d = dice(df_temp['found'], df_temp['actual'])\n",
    "                    yield ['BURST ' + alignment, tp+fp+fn, tp, fp, fn, precision, recall, f1_score, level, site, spearman, pearson, jaccard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_burst =  pd.DataFrame(fetch_burst_results(), columns=COLUMNS)\n",
    "df_burst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_burst.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up the taxonomies for BURST\n",
    "def fetch_bursttaxa_results():\n",
    "    # Columns = [aligner, depth, tp, fp, fn, precision, recall, f1, level, site, spearman, pearson, jaccard]\n",
    "    # Load up the tp/fp\n",
    "    for depth in DEPTHS:\n",
    "        for alignment in ['95', '98']:\n",
    "            file_str = \"../results/simulations_all_{alignment}/combined_seqs.fna_{depth}_*/alignment.burst.b6\".format(depth=depth, alignment=alignment)\n",
    "            infiles = glob.glob(file_str)\n",
    "            for infile in infiles:\n",
    "                f1_results = defaultdict(lambda: defaultdict(int))\n",
    "                with open(infile) as fh:\n",
    "                    for line in fh:\n",
    "                        row = line.rstrip().split(\"\\t\")\n",
    "                        taxa_found = row[-1]\n",
    "                        if 's__' in taxa_found:\n",
    "                            taxa_found = ';'.join(taxa_found.split(';')[:7])\n",
    "                            taxa_actual = title2tax[row[0]]\n",
    "                            group = row[0].split('_')[0]\n",
    "                            if taxa_actual.startswith(taxa_found):\n",
    "                                f1_results[group]['tp'] += 1\n",
    "                            else:\n",
    "                                f1_results[group]['fp'] += 1\n",
    "                folder = '/'.join(infile.split('/')[:-1])\n",
    "                df = pd.read_csv(folder + \"/taxatable.burst.taxonomy.species.txt\", sep=\"\\t\", index_col=0)\n",
    "                df_norm, df_gold = normalize_two_taxatable(df, ground_truth_species)\n",
    "                for group in f1_results.keys():\n",
    "                    taxahit_found = set(df[df[group] > 5].index)\n",
    "                    taxahit_actual = set(ground_truth_species[ground_truth_species[group] > 5].index)\n",
    "                    jaccard = len(taxahit_found.intersection(taxahit_actual))/float(len(taxahit_found.union(taxahit_actual)))\n",
    "                    tp = f1_results[group]['tp']\n",
    "                    fp = f1_results[group]['fp']\n",
    "                    fn = int(DEPTHS_DICT[depth]/3.)-(tp+fp)\n",
    "                    precision = tp/float(tp+fp)\n",
    "                    recall = tp/float(tp+fn)\n",
    "                    f1_score = (precision+recall)/2.\n",
    "                    level = 'species'\n",
    "                    site = group\n",
    "                    series_found = df_norm.T[group + \"_found\"]\n",
    "                    series_actual = df_gold.T[group]\n",
    "                    pearson = series_actual.corr(series_found, method='pearson')\n",
    "                    spearman = series_actual.corr(series_found, method='spearman')\n",
    "#                     df_temp = pd.DataFrame([series_found, series_actual]).T\n",
    "#                     df_temp.columns = ['found', 'actual']\n",
    "#                     df_temp = df_temp.fillna(0)\n",
    "#                     d = dice(df_temp['found'], df_temp['actual'])\n",
    "                    yield ['BURST ' + alignment + \" Taxonomy\", tp+fp+fn, tp, fp, fn, precision, recall, f1_score, level, site, spearman, pearson, jaccard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_burst_taxa =  pd.DataFrame(fetch_bursttaxa_results(), columns=COLUMNS)\n",
    "df_burst_taxa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_burst_taxa.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strain2accession = {}\n",
    "with open(\"../data/references/rep82/rep82.tax\") as inf:\n",
    "    for line in inf:\n",
    "        row = line.rstrip().split()\n",
    "        strain2accession[row[1]] = row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f1_results_utree = defaultdict(lambda: defaultdict(int))\n",
    "# with open(\"../results/simulations_all_test/combined_seqs.fna_fulldepth_1/alignment.utree.tsv\") as inf:\n",
    "#     for line in inf:\n",
    "#         row = line.rstrip().split(\"\\t\")\n",
    "#         if row[1] in strain2accession:\n",
    "#             accession_found = strain2accession[row[1]]\n",
    "#             accession_actual = title2acc[row[0]]\n",
    "#             group = row[0].split('_')[0]\n",
    "#             if accession_found == accession_actual:\n",
    "#                 f1_results_utree[group]['tp'] += 1\n",
    "#             else:\n",
    "#                 f1_results_utree[group]['fp'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up the taxonomies for BURST\n",
    "def fetch_utree_results():\n",
    "    # Columns = [aligner, depth, tp, fp, fn, precision, recall, f1, level, site, spearman, pearson, jaccard]\n",
    "    # Load up the tp/fp\n",
    "    for depth in DEPTHS:\n",
    "        for alignment in ['95']:\n",
    "            file_str = \"../results/simulations_all_{alignment}/combined_seqs.fna_{depth}_*/alignment.utree.tsv\".format(depth=depth, alignment=alignment)\n",
    "            infiles = glob.glob(file_str)\n",
    "            for infile in infiles:\n",
    "                f1_results = defaultdict(lambda: defaultdict(int))\n",
    "                with open(infile) as fh:\n",
    "                    for line in fh:\n",
    "                        row = line.rstrip().split(\"\\t\")\n",
    "                        taxa_found = row[2]\n",
    "                        if row[1] in strain2accession:\n",
    "                            accession_found = strain2accession[row[1]]\n",
    "                            accession_actual = title2acc[row[0]]\n",
    "                            group = row[0].split('_')[0]\n",
    "                            if accession_found == accession_actual:\n",
    "                                f1_results[group]['tp'] += 1\n",
    "                            else:\n",
    "                                f1_results[group]['fp'] += 1\n",
    "                folder = '/'.join(infile.split('/')[:-1])\n",
    "                df = pd.read_csv(folder + \"/taxatable.utree.species.normalized.txt\", sep=\"\\t\", index_col=0)\n",
    "                df_norm, df_gold = normalize_two_taxatable(df, ground_truth_species)\n",
    "                for group in f1_results.keys():\n",
    "                    taxahit_found = set(df[df[group] > 5].index)\n",
    "                    taxahit_actual = set(ground_truth_species[ground_truth_species[group] > 5].index)\n",
    "                    jaccard = len(taxahit_found.intersection(taxahit_actual))/float(len(taxahit_found.union(taxahit_actual)))\n",
    "                    tp = f1_results[group]['tp']\n",
    "                    fp = f1_results[group]['fp']\n",
    "                    fn = int(DEPTHS_DICT[depth]/3.)-(tp+fp)\n",
    "                    precision = tp/float(tp+fp)\n",
    "                    recall = tp/float(tp+fn)\n",
    "                    f1_score = (precision+recall)/2.\n",
    "                    level = 'species'\n",
    "                    site = group\n",
    "                    series_found = df_norm.T[group + \"_found\"]\n",
    "                    series_actual = df_gold.T[group]\n",
    "                    pearson = series_actual.corr(series_found, method='pearson')\n",
    "                    spearman = series_actual.corr(series_found, method='spearman')\n",
    "#                     df_temp = pd.DataFrame([series_found, series_actual]).T\n",
    "#                     df_temp.columns = ['found', 'actual']\n",
    "#                     df_temp = df_temp.fillna(0)\n",
    "#                     d = dice(df_temp['found'], df_temp['actual'])\n",
    "                    yield ['UTree', tp+fp+fn, tp, fp, fn, precision, recall, f1_score, level, site, spearman, pearson, jaccard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_utree =  pd.DataFrame(fetch_utree_results(), columns=COLUMNS)\n",
    "df_utree.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_utree.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shogun.redistribute import Taxonomy\n",
    "from shogun.utils import build_lca_map\n",
    "from shogun.parsers import yield_alignments_from_sam_inf\n",
    "\n",
    "tree = Taxonomy(\"../data/references/rep82/rep82.tax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up the taxonomies for BURST\n",
    "def fetch_bowtie2_results():\n",
    "    # Columns = [aligner, depth, tp, fp, fn, precision, recall, f1, level, site, spearman, pearson, jaccard]\n",
    "    # Load up the tp/fp\n",
    "    for depth in DEPTHS:\n",
    "        for alignment in ['95', '98']:\n",
    "            file_str = \"../results/simulations_all_{alignment}/combined_seqs.fna_{depth}_*/alignment.bowtie2.sam\".format(depth=depth, alignment=alignment)\n",
    "            infiles = glob.glob(file_str)\n",
    "            for infile in infiles:\n",
    "                f1_results = defaultdict(lambda: defaultdict(int))\n",
    "                align_gen = yield_alignments_from_sam_inf(infile)\n",
    "                lca_map = build_lca_map(align_gen, tree)\n",
    "                for key, value in lca_map.items():\n",
    "                    if value:\n",
    "                        if 's__' in value:\n",
    "                            taxa_found = ';'.join(value.split(';')[:7])\n",
    "                            taxa_actual = title2tax[key]\n",
    "                            group = key.split('_')[0]\n",
    "                            if taxa_actual.startswith(taxa_found):\n",
    "                                f1_results[group]['tp'] += 1\n",
    "                            else:\n",
    "                                f1_results[group]['fp'] += 1\n",
    "                folder = '/'.join(infile.split('/')[:-1])\n",
    "                df = pd.read_csv(folder + \"/taxatable.bowtie2.species.normalized.txt\", sep=\"\\t\", index_col=0)\n",
    "                df_norm, df_gold = normalize_two_taxatable(df, ground_truth_species)\n",
    "                for group in f1_results.keys():\n",
    "                    taxahit_found = set(df[df[group] > 5].index)\n",
    "                    taxahit_actual = set(ground_truth_species[ground_truth_species[group] > 5].index)\n",
    "                    jaccard = len(taxahit_found.intersection(taxahit_actual))/float(len(taxahit_found.union(taxahit_actual)))\n",
    "                    tp = f1_results[group]['tp']\n",
    "                    fp = f1_results[group]['fp']\n",
    "                    fn = int(DEPTHS_DICT[depth]/3.)-(tp+fp)\n",
    "                    precision = tp/float(tp+fp)\n",
    "                    recall = tp/float(tp+fn)\n",
    "                    f1_score = (precision+recall)/2.\n",
    "                    level = 'species'\n",
    "                    site = group\n",
    "                    series_found = df_norm.T[group + \"_found\"]\n",
    "                    series_actual = df_gold.T[group]\n",
    "                    pearson = series_actual.corr(series_found, method='pearson')\n",
    "                    spearman = series_actual.corr(series_found, method='spearman')\n",
    "#                     df_temp = pd.DataFrame([series_found, series_actual]).T\n",
    "#                     df_temp.columns = ['found', 'actual']\n",
    "#                     df_temp = df_temp.fillna(0)\n",
    "#                     d = dice(df_temp['found'], df_temp['actual'])\n",
    "                    yield ['Bowtie2 ' + alignment, tp+fp+fn, tp, fp, fn, precision, recall, f1_score, level, site, spearman, pearson, jaccard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_bowtie2 =  pd.DataFrame(fetch_bowtie2_results(), columns=COLUMNS)\n",
    "df_bowtie2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_bowtie2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_burst, df_burst_taxa, df_utree, df_bowtie2])\n",
    "df_all['depth_log10'] = np.round(np.log10(df_all['depth']))\n",
    "df_all.to_csv(\"../figures/simulations_all.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are interested in the speed up and efficiency of each of the aligners\n",
    "# y-axis: Recall\n",
    "# x-axis: N (workers)\n",
    "g = sns.factorplot(x=\"depth_log10\", y=\"recall\", hue=\"aligner\", col=\"site\", data=df_all, palette=COLORS)\n",
    "(g.set_ylabels(\"Recall\").set_xlabels(\"Depth (log10)\"))\n",
    "pltname = \"recall\"\n",
    "save_plot(g, pltname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are interested in the speed up and efficiency of each of the aligners\n",
    "# y-axis: Recall\n",
    "# x-axis: N (workers)\n",
    "g = sns.factorplot(x=\"depth_log10\", y=\"spearman\", hue=\"aligner\", col=\"site\", data=df_all, palette=COLORS)\n",
    "(g.set_ylabels(\"Spearman\").set_xlabels(\"Depth (log10)\"))\n",
    "pltname = \"spearman\"\n",
    "save_plot(g, pltname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['aligner'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are interested in the speed up and efficiency of each of the aligners\n",
    "# y-axis: Recall\n",
    "# x-axis: N (workers)\n",
    "g = sns.factorplot(x=\"depth_log10\", y=\"jaccard\", hue=\"aligner\", col=\"site\", data=df_all, palette=COLORS)\n",
    "(g.set_ylabels(\"Jaccard\").set_xlabels(\"Depth (log10)\"))\n",
    "pltname = \"jaccard\"\n",
    "save_plot(g, pltname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are interested in the speed up and efficiency of each of the aligners\n",
    "# y-axis: Recall\n",
    "# x-axis: N (workers)\n",
    "g = sns.factorplot(x=\"depth_log10\", y=\"f1\", hue=\"aligner\", col=\"site\", data=df_all, palette=COLORS)\n",
    "(g.set_ylabels(\"F1 Micro\").set_xlabels(\"Depth (log10)\"))\n",
    "pltname = \"f1\"\n",
    "save_plot(g, pltname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are interested in the speed up and efficiency of each of the aligners\n",
    "# y-axis: Recall\n",
    "# x-axis: N (workers)\n",
    "g = sns.factorplot(x=\"depth_log10\", y=\"precision\", hue=\"aligner\", col=\"site\", data=df_all, palette=COLORS)\n",
    "(g.set_ylabels(\"Precision\").set_xlabels(\"Depth (log10)\"))\n",
    "pltname = \"precision\"\n",
    "save_plot(g, pltname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up the taxonomies for BURST\n",
    "def fetch_centrifuge_results():\n",
    "    # Columns = [aligner, depth, tp, fp, fn, precision, recall, f1, level, site, spearman, pearson, jaccard]\n",
    "    # Load up the tp/fp\n",
    "    for depth in DEPTHS:\n",
    "        for alignment in ['95']:\n",
    "            file_str = \"../results/simulations_all_{alignment}/combined_seqs.fna_{depth}_*/centrifuge.classification.txt\".format(depth=depth, alignment=alignment)\n",
    "            infiles = glob.glob(file_str)\n",
    "            for infile in infiles:\n",
    "                f1_results = defaultdict(lambda: defaultdict(int))\n",
    "                with open(infile) as inf:\n",
    "                    next(inf)\n",
    "                    for line in inf:\n",
    "                        row = line.rstrip().split(\"\\t\")\n",
    "                        if row[-1] == \"1\":\n",
    "                            accession_found = row[1]\n",
    "                            accession_actual = title2acc[row[0]]\n",
    "                            group = row[0].split('_')[0]\n",
    "                            if accession_found == accession_actual:\n",
    "                                f1_results[group]['tp'] += 1\n",
    "                            else:\n",
    "                                f1_results[group]['fp'] += 1\n",
    "                for group in f1_results.keys():\n",
    "                    tp = f1_results[group]['tp']\n",
    "                    fp = f1_results[group]['fp']\n",
    "                    fn = int(DEPTHS_DICT[depth]/3.)-(tp+fp)\n",
    "                    precision = tp/float(tp+fp)\n",
    "                    recall = tp/float(tp+fn)\n",
    "                    f1_score = (precision+recall)/2.\n",
    "                    level = 'species'\n",
    "                    site = group\n",
    "#                     df_temp = pd.DataFrame([series_found, series_actual]).T\n",
    "#                     df_temp.columns = ['found', 'actual']\n",
    "#                     df_temp = df_temp.fillna(0)\n",
    "#                     d = dice(df_temp['found'], df_temp['actual'])\n",
    "                    yield ['Centrifuge', tp+fp+fn, tp, fp, fn, precision, recall, f1_score, level, site, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_centrifuge = pd.DataFrame(fetch_centrifuge_results(), columns=COLUMNS)\n",
    "df_centrifuge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxid2accession = dict()\n",
    "with open(\"../results/indices/centrifuge_rep82.map\") as fh:\n",
    "    for line in fh:\n",
    "        row = line.rstrip().split('\\t')\n",
    "        taxid2accession[row[1]] = row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up the taxonomies for BURST\n",
    "def fetch_kraken_results():\n",
    "    # Columns = [aligner, depth, tp, fp, fn, precision, recall, f1, level, site, spearman, pearson, jaccard]\n",
    "    # Load up the tp/fp\n",
    "    for depth in DEPTHS:\n",
    "        for alignment in ['95']:\n",
    "            file_str = \"../results/simulations_all_{alignment}/combined_seqs.fna_{depth}_*/kraken.report.txt\".format(depth=depth, alignment=alignment)\n",
    "            infiles = glob.glob(file_str)\n",
    "            for infile in infiles:\n",
    "                f1_results = defaultdict(lambda: defaultdict(int))\n",
    "                with open(infile) as inf:\n",
    "                    for line in inf:\n",
    "                        row = line.rstrip().split(\"\\t\")\n",
    "                        if row[2] in taxid2accession and row[0] == \"C\":\n",
    "                            accession_found = taxid2accession[row[2]]\n",
    "                            accession_actual = title2acc[row[1]]\n",
    "                            group = row[1].split('_')[0]\n",
    "                            if accession_found == accession_actual:\n",
    "                                f1_results[group]['tp'] += 1\n",
    "                            else:\n",
    "                                f1_results[group]['fp'] += 1\n",
    "                for group in f1_results.keys():\n",
    "                    tp = f1_results[group]['tp']\n",
    "                    fp = f1_results[group]['fp']\n",
    "                    fn = int(DEPTHS_DICT[depth]/3.)-(tp+fp)\n",
    "                    precision = tp/float(tp+fp)\n",
    "                    recall = tp/float(tp+fn)\n",
    "                    f1_score = (precision+recall)/2.\n",
    "                    level = 'species'\n",
    "                    site = group\n",
    "#                     df_temp = pd.DataFrame([series_found, series_actual]).T\n",
    "#                     df_temp.columns = ['found', 'actual']\n",
    "#                     df_temp = df_temp.fillna(0)\n",
    "#                     d = dice(df_temp['found'], df_temp['actual'])\n",
    "                    yield ['Kraken', tp+fp+fn, tp, fp, fn, precision, recall, f1_score, level, site, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kraken = pd.DataFrame(fetch_kraken_results(), columns=COLUMNS)\n",
    "df_kraken.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_burst, df_burst_taxa, df_utree, df_bowtie2, df_kraken, df_centrifuge])\n",
    "df_all['depth_log10'] = np.round(np.log10(df_all['depth']))\n",
    "df_all.to_csv(\"../figures/simulations_kraken_centrifuge.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_kraken.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are interested in the speed up and efficiency of each of the aligners\n",
    "# y-axis: Recall\n",
    "# x-axis: N (workers)\n",
    "g = sns.factorplot(x=\"depth_log10\", y=\"f1\", hue=\"aligner\", col=\"site\", data=df_all)\n",
    "(g.set_ylabels(\"F1 Micro\").set_xlabels(\"Depth (log10)\"))\n",
    "pltname = \"f1_kraken_centrifuge\"\n",
    "save_plot(g, pltname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are interested in the speed up and efficiency of each of the aligners\n",
    "# y-axis: Recall\n",
    "# x-axis: N (workers)\n",
    "g = sns.factorplot(x=\"depth_log10\", y=\"precision\", hue=\"aligner\", col=\"site\", data=df_all)\n",
    "(g.set_ylabels(\"Precision\").set_xlabels(\"Depth (log10)\"))\n",
    "pltname = \"precision_kraken_centrifuge\"\n",
    "save_plot(g, pltname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are interested in the speed up and efficiency of each of the aligners\n",
    "# y-axis: Recall\n",
    "# x-axis: N (workers)\n",
    "g = sns.factorplot(x=\"depth_log10\", y=\"recall\", hue=\"aligner\", col=\"site\", data=df_all)\n",
    "(g.set_ylabels(\"Recall\").set_xlabels(\"Depth (log10)\"))\n",
    "pltname = \"recall_kraken_centrifuge\"\n",
    "save_plot(g, pltname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
